{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbca5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "import tiktoken\n",
    "import pickle \n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ff365d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the document to be analysed\n",
    "PDF_DOC = \"data/2023_Q1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "648a118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN AI models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "EMBEDDING_ENCODING = \"cl100k_base\"  # this the encoding for text-embedding-ada-002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebdad35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the text from a pdf file\n",
    "def extract_text_frfom_pdf(doc, npages=None):\n",
    "    reader = PdfReader(doc)  \n",
    "        \n",
    "    n = len(reader.pages)\n",
    "    if npages is not None:\n",
    "        npages = min(npages, n)\n",
    "    else:\n",
    "        npages = n\n",
    "        \n",
    "    print (f'You have {n:,} page(s) in your file, loading {npages:,}')\n",
    "\n",
    "    text = ''\n",
    "    for i in range(npages):\n",
    "        text += reader.pages[i].extract_text()\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc1c2b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_text(full_text, psize=500, delim='\\n'):    \n",
    "    text_chunks = full_text.split(delim)\n",
    "    \n",
    "    ptext = []\n",
    "    next_item = ''\n",
    "    next_item_size = 0\n",
    "    \n",
    "    for i in range(len(text_chunks)):\n",
    "        txt = text_chunks[i]\n",
    "        \n",
    "        next_item_size += len(txt)\n",
    "        next_item += txt + ' '\n",
    "            \n",
    "        if next_item_size>psize:\n",
    "            next_item_size = 0\n",
    "            ptext.append(next_item)\n",
    "            next_item = ''\n",
    "                        \n",
    "    return ptext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4559935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overlapped_partitions(ptext, overlap_pct=0.5, delim=' '):\n",
    "    assert len(ptext)>1, 'Insufficient text to partition'\n",
    "    \n",
    "    opart = []\n",
    "    pos1 = int(float(len(ptext[0])*overlap_pct))\n",
    "\n",
    "    for i in range(1, len(ptext)):\n",
    "        pos2 = int(float(len(ptext[i])*overlap_pct))\n",
    "        \n",
    "        while ptext[i-1][pos1] != delim:\n",
    "            pos1 += 1 \n",
    "        while ptext[i][pos2] != delim:\n",
    "            pos2 += 1\n",
    "        \n",
    "        new_part = ptext[i-1][pos1:]\n",
    "        new_part += ptext[i][:pos2]\n",
    "        opart.append(new_part)\n",
    "        \n",
    "        pos1 = pos2\n",
    "    \n",
    "    return opart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07176410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into partitions, including overlapping partitions\n",
    "def create_text_partitions(full_text, psize=500, overlap_pct=0.5):\n",
    "    ptext = partition_text(full_text, psize=200)\n",
    "    optext = create_overlapped_partitions(ptext, overlap_pct)\n",
    "    lp = len(ptext)\n",
    "    lop = len(optext)\n",
    "    ptext.extend(optext)\n",
    "    nc = len(ptext)\n",
    "    print(f'Loaded {nc:,} chuncks : {lp:,} chuncks and {lop:,} overlap chunks')\n",
    "    return ptext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a8b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a list of texts into embedding vectors\n",
    "def create_embeddings(ptext, save=True, fname='data/embeddings.pkl'):\n",
    "    emb_info = []\n",
    "    encoding = tiktoken.get_encoding(EMBEDDING_ENCODING)\n",
    "    \n",
    "    for i in range(len(ptext)):\n",
    "        txt = ptext[i]\n",
    "        ntokens = len(encoding.encode(txt))\n",
    "        emb = get_embedding(txt, engine=EMBEDDING_MODEL)\n",
    "        \n",
    "        emb_info.append([txt, ntokens, emb])\n",
    "    \n",
    "    if save:\n",
    "        with open(fname, 'wb') as f:\n",
    "            pickle.dump(emb_info, f)\n",
    "    \n",
    "    return emb_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a15b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(fname='data/embeddings.pkl'):\n",
    "    with open(fname, 'rb') as f:\n",
    "        emb_info = pickle.load(f)\n",
    "    \n",
    "    sl = [len(x[2]) for x in emb_info]\n",
    "    assert max(sl)==min(sl), 'incompatible embedding sizes'\n",
    "    print(f'Loaded {len(sl):,} embeddings, each of size {max(sl):,}')\n",
    "    return emb_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f7bac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the embedding vector of a query with the \n",
    "# embedding vectors corresponding to document chuncks\n",
    "def find_similar_text(query, emb, sim_threshold=0.8):\n",
    "    qe = get_embedding(query, engine=EMBEDDING_MODEL)\n",
    "    sim = [cosine_similarity(e[2], qe) for e in emb]\n",
    "    \n",
    "    res_info = []\n",
    "    sim_text = ''\n",
    "    for i in range(len(emb)):\n",
    "        if sim[i]>=sim_threshold:\n",
    "            res_info.append([emb[i][0], emb[i][1], emb[i][2], sim[i]])\n",
    "            sim_text += emb[i][0] + ' '\n",
    "            \n",
    "    return sim_text, res_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57fe7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a whole \"prompt engineering\" field\n",
    "# this is a super simple way to create a gpt prompt\n",
    "def get_prompt(question, doc_specific=True, doc_text=''):\n",
    "    if doc_specific:\n",
    "        prompt = 'The document provided contains the following information: ' \\\n",
    "                + doc_text + ' ' + question\n",
    "        role_descr = 'You answer questions about the document provided. If the information is not in the document say you do not know the answer.'\n",
    "    else:\n",
    "        prompt = question\n",
    "        role_descr = 'You answer the question asked.'\n",
    "        \n",
    "    return prompt, role_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98fbc929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the main function\n",
    "def ask_question(question, # text of the question \n",
    "                 doc_embeddings, # the embeddings of the PDF\n",
    "                 doc_specific=True, # makes the answer doc specific only\n",
    "                 verbose=True, \n",
    "                 sim_threshold=0.8 # threshold to determine what embeddings to use in the prompt\n",
    "                ):\n",
    "    similar_text, sim_info = find_similar_text( question, \n",
    "                                                doc_embeddings,\n",
    "                                                sim_threshold)\n",
    "    \n",
    "    prompt, role_descr = get_prompt(question, doc_specific, similar_text)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": role_descr},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    \n",
    "    response_info = openai.ChatCompletion.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "        )\n",
    "    \n",
    "    response = response_info['choices'][0]['message']['content']\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Answer: {response}')\n",
    "    \n",
    "    return response, response_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "145f9e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 40 page(s) in your file, loading 40\n",
      "Loaded 533 chuncks : 267 chuncks and 266 overlap chunks\n"
     ]
    }
   ],
   "source": [
    "text = extract_text_frfom_pdf(PDF_DOC)\n",
    "ptext = create_text_partitions(text, psize=200, overlap_pct=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f236ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to do this once\n",
    "# emb = create_embeddings(ptext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c365550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 533 embeddings, each of size 1,536\n"
     ]
    }
   ],
   "source": [
    "emb = load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acdc7b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'What was the first quarter 2023 revenue?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4727feaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: I'm sorry, but as an AI language model, I do not have access to real-time financial data. Please provide more context or specify the company you are referring to.\n"
     ]
    }
   ],
   "source": [
    "answer, info = ask_question(q, emb, doc_specific=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a03fc17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The first quarter 2023 revenue was $21.4 billion.\n"
     ]
    }
   ],
   "source": [
    "answer, info = ask_question(q, emb, doc_specific=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a01b4762",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'What was the annual tax rate?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aac8f8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The document states that the full year effective tax rate for 2023 is expected to be around 23-24%, excluding discrete items and divestiture-related impacts. However, it does not provide information on the annual tax rate for any previous year.\n"
     ]
    }
   ],
   "source": [
    "answer, info = ask_question(q, emb, doc_specific=True, sim_threshold=0.78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e997c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
